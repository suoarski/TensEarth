{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "protecting-opening",
   "metadata": {},
   "source": [
    "In this notebook, we will discuss some of the underlying theory of neural networks relevant to our work.\n",
    "\n",
    "# Simple Neural Networks\n",
    "\n",
    "We will introduce some basic concepts of neural networks by using logistics regression as an example. They are often considered to be the simplest neural networks with only a single layer of weights. Consider the task of classifying greyscale images of written digits. The pixels in the image are represented in flattened arrays $\\textbf{x} = [x_0, x_1, ..., x_n]$ with numbers between $[0, 1]$, and the output is an array of probabilities $\\textbf{y} = [y_0, y_1, ..., y_n]$ that the image should be classified as a particular digit.\n",
    "\n",
    "### Basic Structure\n",
    "The input $\\textbf{x}$ is often refered to as **features**, and the output $\\textbf{y}$ as **targets**. In the case of logistics regression, each feature node $x_i$ is connected to each target node $y_j$ and has an associated weight of $w_{ij}$, whose value will be determined during the training stage of the neural network. The targets are then calculated by:\n",
    "\n",
    "$$ \\hat{y}_j = \\sigma \\Bigg( \\sum_{i=0} w_{ij} x_i \\Bigg) $$\n",
    "\n",
    "where $\\sigma$ is the **activation** function, which brings the sum back into the range of $[0, 1]$. Although various activation functions exists, a popular one is the sigmoid given by:\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "In the case of logistics regression, we only have one of such layers, but we can introduce multiple layers and get a deep neural network. In tensorflow, these types of layers are called **Dense** layers and sometimes they are refered to as **fully connected** layers.\n",
    "\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"NotebookImages/LogisticsRegression.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "### Training the Network\n",
    "\n",
    "In order to train a neural network, we first need to define a **loss** or **error** function. The loss function is a measure of how badly a model is performing, and training is achieved by minimizing the loss function using training examples. In the case of digit classification, we have a data set of features (images) with their corresponding known target values (Eg. $\\textbf{y} = [0, 1, 0, ..., 0]$). Given a model prediction $\\hat{\\textbf{y}}$, and the true known target $\\textbf{y}$, a simple example loss function $\\ell$ would be the mean square error:\n",
    "$$ \\ell (\\textbf{y}, \\hat{\\textbf{y}}) = \\frac{1}{n} \\sum_{k=0} (y_i - \\hat{y}_k)^2 $$\n",
    "\n",
    "Note that choosing a suitable loss function is essential in specifying what the neural network is supposed to learn. [Here is a list](https://www.tensorflow.org/api_docs/python/tf/keras/losses) of built in loss functions that come with tensorflow. In this project, we might also be interested in designing our own Physics Informed Neural Network (PINN), where we include a physics based equation into the loss function to punish the model for not respecting a particular relationship of our output/input variables.\n",
    "\n",
    "In designing the neural network, we make sure that it's predictions $\\hat{\\textbf{y}}$ are differentiable with respect to it's weights $w_{ij}$, which means our loss function is also differentiable with respect to all weights. This allows us to use various **optimization algorithms** to adjust all our weights in order to minimize the **loss** of our model. A popular optimizer is **Adam** (Adaptive Moment Estimation) which we will use extensively in this project, and [here is a list](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) of built in optimizers that come with tensorflow.\n",
    "\n",
    "\n",
    "### Convolutional Layers\n",
    "In our work, we will be making heavy use on convolutional layers in our models, so we will discuss them here. The animated GIF bellow was [taken from this website](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53), and provides a really usefull visualization of convolutional layers. \n",
    "\n",
    "[This youtube video](https://youtu.be/8rrHTtUzyZA) also does a really good job at explaining convolutions, although not necessarily in the context of Convolutional Neural Networks (CNN).\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"NotebookImages/Convolute.gif\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "In a convolutional layer, the input array needs to have at least 2 dimensions to convolve on (like an image), and an optional third dimension of arbitrary length can be also included (Eg. Color Channels or Output from previous convolutional layers). Instead of directly connecting nodes with weights, we instead create multiple kernels that will scan the image as shown in the animation above.\n",
    "\n",
    "The kernel of a convolution is essentially a small matrix of weights that traverses the image as shown above. For each pixel, the kernel will scalar multiply itself with the neighbouring pixels. The sum is then passed through an activation function (like the sigmoid), which will provide the pixel value of the output image. During training, we the weights of the kernel matrices will be adjusted.\n",
    "\n",
    "**Pooling** layers are also often used in combination with convolutional layers, to reduce the dimensionality of the output. This can be an effective tool for ignore noise in an image. The image bellow taken from the same website demonstrates max and average pooling. Here an $N \\times N$ image is reduced to an $\\frac{N}{2} \\times \\frac{N}{2}$ image, by taking the maximum or average of closely grouped pixels.\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"NotebookImages/Pooling.jpg\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-champion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-preserve",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-ballet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-induction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
