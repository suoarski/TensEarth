{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7827e01",
   "metadata": {},
   "source": [
    "# Running This Notebook\n",
    "\n",
    "To run this notebook, you will need to have [tensorflow](https://www.tensorflow.org/install/docker) installed. When installing tensorflow, it is optional whether or not you want to use GPU to significantly speed up computations. This is not absolutely necessary to simply run this notebook, the code will run fine, but highly recomended if you plan on running the training stage of the model. You may get warning messages from Tensorflow saying that Cuda (GPU) is not enabled, but you can ignore those.\n",
    "\n",
    "I reccomend running this notebook in [this](https://hub.docker.com/r/tensorflow/tensorflow/) docker environment, with jupyter and GPU enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd0f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c8b48",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "When tensorflow reads images, it likes to keep them in tensors that are batches of images. Batches are used to speed up the training of models. The tensors from batches are larger, but fewer tensor operations need to be done.\n",
    "\n",
    "Also note, that unlike numpy arrays, tensorflow data sets are not directly stored in memory, as they are sometimes too big. Instead they specify a data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for reading data from our image directories\n",
    "def getBatchDataset(imageDir, imageHeight, imageWidth, batch_size):\n",
    "    return tf.keras.utils.image_dataset_from_directory(\n",
    "        imageDir,\n",
    "        labels=None,\n",
    "        seed=123,\n",
    "        image_size=(imageHeight, imageWidth),\n",
    "        batch_size=batch_size)\n",
    "\n",
    "#Image parameters\n",
    "batch_size = 4\n",
    "imageWidth = 256\n",
    "imageHeight = 512\n",
    "\n",
    "#Choose image data directory depending whether we want to use all data, or a small subset for debugging.\n",
    "useAllData = False\n",
    "if useAllData:\n",
    "    featuresDir = './Data/ImageTrainingData/Features'\n",
    "    targetsDir = './Data/ImageTrainingData/Targets'\n",
    "else:\n",
    "    featuresDir = './Data/SmallImageTrainingDataset/Features'\n",
    "    targetsDir = './Data/SmallImageTrainingDataset/Targets'\n",
    "\n",
    "#Load data, extract a batch of each dataset, and extract an image from each batch\n",
    "featureData = getBatchDataset(featuresDir, imageHeight, imageWidth, batch_size)\n",
    "targetData = getBatchDataset(targetsDir, imageHeight, imageWidth, batch_size)\n",
    "\n",
    "featureBatch = next(iter(featureData))\n",
    "targetBatch = next(iter(targetData))\n",
    "\n",
    "featureImage = featureBatch[2].numpy()\n",
    "targetImage = targetBatch[2].numpy()\n",
    "\n",
    "#Plot Training Data\n",
    "fig, axis = plt.subplots(1, 2)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10)\n",
    "axis[0].imshow(featureImage.astype(np.uint8))\n",
    "axis[1].imshow(targetImage.astype(np.uint8))\n",
    "axis[0].set_title('Current Iteration')\n",
    "axis[1].set_title('Next Iteration')\n",
    "axis[0].set_xticks([])\n",
    "axis[0].set_yticks([])\n",
    "axis[1].set_xticks([])\n",
    "axis[1].set_yticks([])\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055cf6cc",
   "metadata": {},
   "source": [
    "# First Attempt at Making a GAN\n",
    "\n",
    "Here we walk through our first attempt at making a Generative Adversarial Network to simulate erosion. A good amount of this section is based on [this GAN tutorial for tensorflow](https://www.tensorflow.org/tutorials/generative/dcgan), but is modified to suit our data.\n",
    "\n",
    "Although the first attempt gave terrible results, it did help me learn the basics of making a GAN and helped me be more familiar with more advanced tensorflow models. I'll keep the code here amyways, to demonstrate the progress of this project.\n",
    "\n",
    "### Generator Model\n",
    "\n",
    "The generator model bellow is based the tensorflow tutorial that I was following, with some adjustments made to better suit it our data, and further changes made based on trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc5f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we define the structure of the generator model\n",
    "def getGeneratorModel(imageWidth, imageHeight):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    #A few convolution layers for interpreting input image\n",
    "    model.add(layers.Conv2D(64, 4, activation='relu'))\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(32, 3, activation='relu'))\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    model.add(layers.Dense(16 * 8 * 3, activation='relu'))\n",
    "    model.add(layers.Reshape((16, 8, 3)))\n",
    "    \n",
    "    #Some layers for generating images\n",
    "    model.add(layers.Conv2DTranspose(64, (10, 10), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(32, (10, 10), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(16, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(8, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    #Final output layer\n",
    "    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b7e4c",
   "metadata": {},
   "source": [
    "As of now, our model has not been trained yet, so it's output should be a noisy image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0672e11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate some random results with our untrained model\n",
    "batch = next(iter(featureData))\n",
    "generator = getGeneratorModel(imageWidth, imageHeight)\n",
    "generatedImageBatch = generator(batch, training=False)\n",
    "\n",
    "#Check that the output images has the desired dimensions\n",
    "print(batch.shape)\n",
    "print(generatedImageBatch.shape)\n",
    "\n",
    "image = generatedImageBatch[2].numpy()\n",
    "image /= np.max(image)\n",
    "image *= 255\n",
    "\n",
    "#Visualize results\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(image.astype(np.uint8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db6fa57",
   "metadata": {},
   "source": [
    "### Discriminator Model\n",
    "\n",
    "The discriminator model will try to distinguish between generated and fake images. It returns a value between $[-1, 1]$ depending on how confident it is that the image is real $1$ or fake $-1$ (created by our generator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286440ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDiscriminatorModel(imageWidth, imageHeight):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    #Two convolutional layers\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #Output either a 1 or 0 for real or fake\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a41be5a",
   "metadata": {},
   "source": [
    "Using our image generated from our untrained generator network, we test that our untrained discriminator model is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = getDiscriminatorModel(imageWidth, imageHeight)\n",
    "decision = discriminator(generatedImageBatch)\n",
    "print(decision[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b17680",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "Loss functions in machine learning are used to measure how well a particular model is performing. During the training stage, our model will adjust it's parameters in an attempt to minimize it's loss function. We will need a loss function for both the generator and discriminator models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8582a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "#Checkpoints incase our model training is interrupted\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc5eaea",
   "metadata": {},
   "source": [
    "### Defining the Training Loop\n",
    "\n",
    "Here we specify the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52779c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop of GAN\n",
    "@tf.function\n",
    "def train_step(inputBatch, outputBatch):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        \n",
    "        #Generate next simulation iterator using our generator model\n",
    "        generatedImages = generator(inputBatch, training=True)\n",
    "        \n",
    "        #Pass real and fake images to discriminator\n",
    "        real_output = discriminator(outputBatch, training=True)\n",
    "        fake_output = discriminator(generatedImages, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1120ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(featureData, targetData, epochs, generator):\n",
    "    testBatch = next(iter(featureData))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        for inputBatch, outputBatch in zip(featureData, targetData):\n",
    "            train_step(inputBatch, outputBatch, generator)\n",
    "\n",
    "        # Produce images for the GIF as you go\n",
    "        generate_and_save_images(generator, epoch + 1, testBatch)\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    generate_and_save_images(generator, epochs, testBatch)\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    predictions = model(test_input, training=False)\n",
    "    plt.imshow(predictions[0].numpy())\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6691c2f6",
   "metadata": {},
   "source": [
    "Change to true to train the model. If you don't feel like waiting for the training to finish however, we provide some results images bellow. As we can see, the inital results after our first attempt are really bad, and does not show erosion at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af03680",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    epochs = 5\n",
    "    train(featureData, targetData, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f213e1",
   "metadata": {},
   "source": [
    "Example results image:\n",
    "<div>\n",
    "<img src=\"Data/ResultsImages/Trial1/image_at_epoch_0003.png\" width=\"900\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e44d348",
   "metadata": {},
   "source": [
    "As we can see, our first attempt gave us really bad results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f4af42",
   "metadata": {},
   "source": [
    "# GAN Model Based on Eric Guerin's Paper\n",
    "\n",
    "### Generator Model\n",
    "After testing, the above model did not give desirable results. To fix this, we will see how the following GAN by [Eric Guerin et. al.](https://hal.archives-ouvertes.fr/hal-01583706/file/tog.pdf) performs on our data. We will modify various parameters and layers to make the GAN work with our data.\n",
    "\n",
    "- Just like our above model, they implement a encoder layer using a sequence of convolutional layers, and an decoder with a sequence of deconvolutional layers.\n",
    "- In addition, they also use skip layers to connect the convolutional layers with deconvolutional layers of the same resolution.\n",
    "- Their graphics card has 12 Gb of dedicated graphics RAM, I only have 8 Gb, so I may have to work around this. I should still be able to get reasonable results though.\n",
    "\n",
    "The code bellow is copied and pasted from the [*nanoxas/sketch-to-terrain* Github](https://github.com/nanoxas/sketch-to-terrain/blob/master/model.py), and adjusted for our purposes. The original code is the same code provided by Eric Guerin et. al's paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6b01fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we define the structure of the generator model\n",
    "def getInspiredGeneratorModel(imageWidth, imageHeight):\n",
    "    \n",
    "    #Input layer\n",
    "    inputs = layers.Input((imageHeight, imageWidth, 3))\n",
    "    \n",
    "    #A few convolutional layers with maxpooling\n",
    "    #This is the encoder part of the model that interprets the input image\n",
    "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    #Their last convolution layer in their encoder seems to have skip layer with noise introduced to it\n",
    "    #Noise is often used to avoid the overfiting of a machine learning model\n",
    "    noise = layers.Input((K.int_shape(conv5)[1], K.int_shape(conv5)[2], K.int_shape(conv5)[3]))\n",
    "    conv5 = layers.Concatenate()([conv5, noise])\n",
    "    \n",
    "    #From my understanding, upsampling is used to increase the weight of data in the minority class\n",
    "    #Skip layer (connects conv4 layer directly to up6 layer), and more convolutional layers\n",
    "    up6 = layers.Conv2D(512, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv5))\n",
    "    merge6 = layers.Concatenate()([conv4, up6])\n",
    "    conv6 = layers.Conv2D(512, 3, activation='relu', padding='same')(merge6)\n",
    "    conv6 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
    "    \n",
    "    up7 = layers.Conv2D(256, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv6))\n",
    "    merge7 = layers.Concatenate()([conv3, up7])\n",
    "    conv7 = layers.Conv2D(256, 3, activation='relu', padding='same')(merge7)\n",
    "    conv7 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    up8 = layers.Conv2D(128, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv7))\n",
    "    merge8 = layers.Concatenate()([conv2, up8])\n",
    "    conv8 = layers.Conv2D(128, 3, activation='relu', padding='same')(merge8)\n",
    "    conv8 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
    "    \n",
    "    up9 = layers.Conv2D(64, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv8))\n",
    "    merge9 = layers.Concatenate()([conv1, up9])\n",
    "    conv9 = layers.Conv2D(64, 3, activation='relu', padding='same')(merge9)\n",
    "    conv9 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
    "    conv9 = layers.Conv2D(32, 3, activation='relu', padding='same')(conv9)\n",
    "    conv10 = layers.Conv2D(3, 1, activation='tanh')(conv9)\n",
    "    \n",
    "    #Create and return the final model\n",
    "    model = Model(inputs=[inputs, noise], outputs=conv10)\n",
    "    return model\n",
    "\n",
    "#Generate some random results with our untrained model\n",
    "batch = next(iter(featureData))\n",
    "generator = getInspiredGeneratorModel(imageWidth, imageHeight)\n",
    "generator.summary()\n",
    "noise = np.random.normal(0, 1, (batch.shape[0], 32, 16, 1024))\n",
    "generatedImageBatch = generator([batch, noise], training=False)\n",
    "\n",
    "#Check that the output images has the desired dimensions\n",
    "print(batch.shape)\n",
    "print(generatedImageBatch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efa66c3",
   "metadata": {},
   "source": [
    "From the output bellow, we can clearly see the output of the untrained Unet model above maintains many of the features of the input data, which is definetely something we will want. Note that considering that the model has not been trained yet, the results already look much more promising compared to our first attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c6da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot both input and output image examples of our model\n",
    "inputImage = batch[2].numpy()\n",
    "outImage = generatedImageBatch[2].numpy() * 256\n",
    "\n",
    "fig, axis = plt.subplots(1, 2)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10)\n",
    "axis[0].imshow(inputImage.astype(np.uint8))\n",
    "axis[1].imshow(outImage.astype(np.uint8))\n",
    "axis[0].set_title('Input Image')\n",
    "axis[1].set_title('Output Image')\n",
    "axis[0].set_xticks([])\n",
    "axis[0].set_yticks([])\n",
    "axis[1].set_xticks([])\n",
    "axis[1].set_yticks([])\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9e42b0",
   "metadata": {},
   "source": [
    "In order to read the output images as data, we want to avoid any borders associated with the above *plt.imshow()* plot. The code bellow lets us save the data as appropriate images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd887f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for outputing results as an image (and not an imshow plot)\n",
    "image = generatedImageBatch[2].numpy() * 256\n",
    "img = Image.fromarray(image.astype(np.uint8))\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42ca2d0",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Note that the above images are the results of passing our input data through the yet un-trained generator model. The model currently has 31,050,243 trainable parameters, which are initiated by random numbers. Notice how the output images do indeed have a resamblance to the input images, which is a result of the skip layers. This resamblance leads me to believe that the model is clearly capable of learning our training data.\n",
    "\n",
    "From the code bellow, we can see that passing a batch of 32 images through our model takes about 0.015 seconds using my GTX 2080 GPU. This means we can expect our final algorithm (after training) to take less than half a milisecond per iteration to simulate erosion. If this works as intended, then this will be by far the fastest erosion algorithm that I am aware of, by multiple orders of magnitude.\n",
    "\n",
    "Note that if your installation of tensorflow does not have access to a GPU, then your code might take significantly longer to run this cell with a CPU instead. For me, a batch of 32 images takes about 27 seconds to run on my CPU, less than 1 second per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fd8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "noise = np.random.normal(0, 1, (batch.shape[0], 32, 16, 1024))\n",
    "generatedImageBatch = generator([batch, noise], training=False)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c46c02",
   "metadata": {},
   "source": [
    "The code bellow is the original code from [Eric Guerin et. al.'s](https://github.com/nanoxas/sketch-to-terrain/blob/master/model.py) github.\n",
    "\n",
    "<details>\n",
    "<summary> \n",
    "    <p style=\"font-style: italic; color:blue;\">\n",
    "        Click here to see original code\n",
    "    </p>\n",
    "</summary>\n",
    "<br> <p style=\"font-style: italic;\">\n",
    "\n",
    "    from keras.layers import *\n",
    "    from keras.models import Model\n",
    "    from keras.initializers import RandomNormal\n",
    "    import keras.backend as K\n",
    "\n",
    "    def UNet(shape):\n",
    "\n",
    "        inputs = Input(shape)\n",
    "        conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "        conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "        conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "        conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "        conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "        conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "        pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "        conv5 = Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
    "        conv5 = Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
    "        noise = Input((K.int_shape(conv5)[1], K.int_shape(conv5)[2], K.int_shape(conv5)[3]))\n",
    "        conv5 = Concatenate()([conv5, noise])\n",
    "\n",
    "        up6 = Conv2D(\n",
    "            512,\n",
    "            2,\n",
    "            activation='relu',\n",
    "            padding='same')(\n",
    "            UpSampling2D(\n",
    "                size=(\n",
    "                    2,\n",
    "                    2))(conv5))\n",
    "        merge6 = Concatenate()([conv4, up6])\n",
    "        conv6 = Conv2D(512, 3, activation='relu', padding='same')(merge6)\n",
    "        conv6 = Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
    "\n",
    "        up7 = Conv2D(\n",
    "            256,\n",
    "            2,\n",
    "            activation='relu',\n",
    "            padding='same')(\n",
    "            UpSampling2D(\n",
    "                size=(\n",
    "                    2,\n",
    "                    2))(conv6))\n",
    "        merge7 = Concatenate()([conv3, up7])\n",
    "        conv7 = Conv2D(256, 3, activation='relu', padding='same')(merge7)\n",
    "        conv7 = Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
    "\n",
    "        up8 = Conv2D(\n",
    "            128,\n",
    "            2,\n",
    "            activation='relu',\n",
    "            padding='same')(\n",
    "            UpSampling2D(\n",
    "                size=(\n",
    "                    2,\n",
    "                    2))(conv7))\n",
    "        merge8 = Concatenate()([conv2, up8])\n",
    "        conv8 = Conv2D(128, 3, activation='relu', padding='same')(merge8)\n",
    "        conv8 = Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
    "\n",
    "        up9 = Conv2D(\n",
    "            64,\n",
    "            2,\n",
    "            activation='relu',\n",
    "            padding='same')(\n",
    "            UpSampling2D(\n",
    "                size=(\n",
    "                    2,\n",
    "                    2))(conv8))\n",
    "        up9 = ZeroPadding2D(((0, 1), (0, 1)))(up9)\n",
    "        merge9 = Concatenate()([conv1, up9])\n",
    "        conv9 = Conv2D(64, 3, activation='relu', padding='same')(merge9)\n",
    "        conv9 = Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
    "        conv9 = Conv2D(32, 3, activation='relu', padding='same')(conv9)\n",
    "        conv10 = Conv2D(1, 1, activation='tanh')(conv9)\n",
    "\n",
    "        model = Model(input=[inputs, noise], output=conv10)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "</p></details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb523b7",
   "metadata": {},
   "source": [
    "### Discriminator Model\n",
    "\n",
    "**The following section is not finished**\n",
    "\n",
    "Similarly, we will use a discriminator model taken from Eric Guerin's paper, and see how well it performs. According to their paper, their discriminator model returns decisions for multiple patches of a single input image. Which is a bit different to what we had before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f125a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInspiredDiscriminatorModel(imageHeight, imageWidth):\n",
    "    \n",
    "    #Create inputs of initial and generated image to discrimate, and combine them\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    initialImage = layers.Input(shape=(imageHeight, imageWidth, 3))\n",
    "    generatedImage = layers.Input((imageHeight, imageWidth, 3))\n",
    "    combinedImages = layers.Concatenate()([initialImage, generatedImage])\n",
    "    \n",
    "    #Main structure of the discriminator neural network model\n",
    "    d = layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(combinedImages)\n",
    "    d = layers.LeakyReLU(alpha=0.2)(d)\n",
    "    d = layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(d)\n",
    "    d = layers.LeakyReLU(alpha=0.2)(d)\n",
    "    d = layers.Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(d)\n",
    "    d = layers.LeakyReLU(alpha=0.2)(d)\n",
    "    d = layers.Conv2D(512, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(d)\n",
    "    d = layers.LeakyReLU(alpha=0.2)(d)\n",
    "    d = layers.Conv2D(512, (4, 4), padding='same', kernel_initializer=init)(d)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(d)\n",
    "    output = layers.Conv2D(1, (4, 4), padding='same', activation='sigmoid', kernel_initializer=init)(d)\n",
    "    \n",
    "    #Create the model and return it\n",
    "    model = Model([initialImage, generatedImage], output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b767a00",
   "metadata": {},
   "source": [
    "We test that the discriminator model gives a desirable output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e231b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator = getInspiredGeneratorModel(imageWidth, imageHeight)\n",
    "discriminator = getInspiredDiscriminatorModel(imageHeight, imageWidth)\n",
    "#discriminator.summary()\n",
    "\n",
    "batch = next(iter(featureData))\n",
    "noise = np.random.normal(0, 1, (batch.shape[0], 32, 16, 1024))\n",
    "generatedImageBatch = generator([batch, noise], training=False)\n",
    "decision = discriminator((batch, generatedImageBatch))\n",
    "\n",
    "print(decision.shape)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c33973",
   "metadata": {},
   "source": [
    "Their code also contains the following third model, which was not mentioned in their paper. My best guess is that it combines both generator and discriminator models into one combined tensorflow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7dca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mount_discriminator_generator(generator, discriminator, image_shape):\n",
    "    discriminator.trainable = False\n",
    "    input_gen = layers.Input(shape=image_shape)\n",
    "    input_noise = layers.Input(shape=(32, 16, 1024))\n",
    "    gen_out = generator([input_gen, input_noise])\n",
    "    output_d = discriminator([gen_out, input_gen])\n",
    "    model = Model(inputs=[input_gen, input_noise], outputs=[output_d, gen_out])\n",
    "    return model\n",
    "\n",
    "#Create a mountDiscriminator and print summary of model\n",
    "mountDiscriminator = mount_discriminator_generator(generator, discriminator, (imageHeight, imageWidth, 3))\n",
    "mountDiscriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e415ab27",
   "metadata": {},
   "source": [
    "# Training Loop According to Paper\n",
    "\n",
    "The code bellow defines the GAN training loop based on the paper that I'm following. Note that we will output images generated by the GAN during the training stage to help visualize the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbe75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert TF batchdataset into a numpy array\n",
    "def batchDatasetToNumpy(dataset):\n",
    "    dataNP = []\n",
    "    for batch in dataset:\n",
    "        for dat in batch:\n",
    "            dataNP.append(dat.numpy())\n",
    "    dataNP = np.array(dataNP)\n",
    "    dataNP /= np.max(dataNP)\n",
    "    return dataNP\n",
    "\n",
    "#Randomly select a portion of feature data along with their corresponding target data\n",
    "def generateRealSamples(features, targets, n_samples, nXPatches, nYPatches):\n",
    "    idx = np.random.randint(0, features.shape[0], n_samples)\n",
    "    feats = features[idx]\n",
    "    targs = targets[idx]\n",
    "    desiredDiscriminatorProbs = np.ones((n_samples, nXPatches, nYPatches, 1))\n",
    "    return feats, desiredDiscriminatorProbs, targs\n",
    "\n",
    "#Generate fake images corresponding to the real targets\n",
    "def generateFakeSamples(generator, dataset, nXPatches, nYPatches):\n",
    "    w_noise = np.random.normal(0, 1, (dataset.shape[0], 32, 16, 1024))\n",
    "    fakeOutput = generator.predict([dataset, w_noise])\n",
    "    desiredDescriminatorProbs = np.zeros((len(fakeOutput), nXPatches, nYPatches, 1))\n",
    "    return fakeOutput, desiredDescriminatorProbs\n",
    "\n",
    "#Using the test feature image, we save generator output images to view the progress of it's training\n",
    "def saveProgressImage(generator, testImage, iteration, dirName='./ProgressImages/Progress{}.png'):\n",
    "    w_noise = np.random.normal(0, 1, (1, 32, 16, 1024))\n",
    "    generatedImage = generator([firstImage[np.newaxis], w_noise], training=False)[0]\n",
    "    generatedImage = generatedImage.numpy()\n",
    "    generatedImage -= np.min(generatedImage)\n",
    "    generatedImage /= np.max(generatedImage)\n",
    "    generatedImage *= 256\n",
    "\n",
    "    img = Image.fromarray(generatedImage.astype(np.uint8))\n",
    "    img.save(dirName.format(iteration))\n",
    "\n",
    "#Convert data sets to numpy arrays\n",
    "featureNP = batchDatasetToNumpy(featureData)\n",
    "targetNP = batchDatasetToNumpy(targetData)\n",
    "\n",
    "#Chose a random selection of images for this batch, and generate required data for training\n",
    "feats, desiredDiscriminatorProbs, targs = generateRealSamples(featureNP, targetNP, batch_size, 32, 16)\n",
    "fakeOutput, desiredDescriminatorProbs = generateFakeSamples(generator, feats, 32, 16)\n",
    "\n",
    "print(feats.shape)\n",
    "print(targs.shape)\n",
    "print(fakeOutput.shape)\n",
    "print(desiredDiscriminatorProbs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba1ff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Image parameters\n",
    "batch_size = 4\n",
    "imageWidth = 256\n",
    "imageHeight = 512\n",
    "\n",
    "#Choose image data directory depending whether we want to use all data, or a small subset for debugging.\n",
    "useAllData = True\n",
    "if useAllData:\n",
    "    featuresDir = './Data/ImageTrainingData/Features'\n",
    "    targetsDir = './Data/ImageTrainingData/Targets'\n",
    "else:\n",
    "    featuresDir = './Data/SmallImageTrainingDataset/Features'\n",
    "    targetsDir = './Data/SmallImageTrainingDataset/Targets'\n",
    "\n",
    "#Load data and extract a batch of each dataset\n",
    "featureData = getBatchDataset(featuresDir, imageHeight, imageWidth, batch_size)\n",
    "targetData = getBatchDataset(targetsDir, imageHeight, imageWidth, batch_size)\n",
    "featureNP = batchDatasetToNumpy(featureData)\n",
    "targetNP = batchDatasetToNumpy(targetData)\n",
    "featureBatch = next(iter(featureData))\n",
    "targetBatch = next(iter(targetData))\n",
    "\n",
    "#Adam optimizer is a popular algorithm for training neural network models with\n",
    "adamOptimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "#Get and setup generator, discriminator and composite models\n",
    "generator = getInspiredGeneratorModel(imageWidth, imageHeight)\n",
    "discriminator = getInspiredDiscriminatorModel(imageHeight, imageWidth)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=adamOptimizer)\n",
    "compositeModel = mount_discriminator_generator(generator, discriminator, (imageHeight, imageWidth, 3))\n",
    "compositeModel.compile(loss=['binary_crossentropy', 'mae'], loss_weights=[1, 3], optimizer=adamOptimizer)\n",
    "\n",
    "\n",
    "nEpochs = 5 #Number of times to repeat overall training loop\n",
    "batchSize = 4 #n_batch in original code\n",
    "batchesPerEpochs = int(len(featureData) / batchSize)\n",
    "nSteps = nEpochs * batchesPerEpochs\n",
    "\n",
    "avg_loss = 0\n",
    "avg_dloss = 0\n",
    "\n",
    "#Get number of patches in X and Y directions generated by our discriminator\n",
    "batch = next(iter(featureData))\n",
    "w_noise = np.random.normal(0, 1, (batch_size, 32, 16, 1024))\n",
    "generatedImageBatch = generator([batch, w_noise], training=False)\n",
    "decision = discriminator((batch, generatedImageBatch))\n",
    "nXPatches, nYPatches = decision.shape[1], decision.shape[2]\n",
    "\n",
    "#Save a generated image to track progress before training\n",
    "firstImage = featureNP[0]\n",
    "saveProgressImage(generator, firstImage, 0)\n",
    "\n",
    "#Main training loop\n",
    "if False:\n",
    "    for i in range(nSteps):\n",
    "\n",
    "        #Chose a random selection of images for this batch, and generate required data for training\n",
    "        feats, realLabels, targs = generateRealSamples(featureNP, targetNP, batchSize, nXPatches, nYPatches)\n",
    "        fakeOutput, fakeLabels = generateFakeSamples(generator, feats, nXPatches, nYPatches)\n",
    "\n",
    "        w_noise = np.random.normal(0, 1, (batch_size, 32, 16, 1024))\n",
    "        losses_composite = compositeModel.train_on_batch([feats, w_noise], [realLabels, targs])\n",
    "\n",
    "        loss_discriminator_fake = discriminator.train_on_batch([fakeOutput, feats], fakeLabels)\n",
    "        loss_discriminator_real = discriminator.train_on_batch([targs, feats], realLabels)\n",
    "\n",
    "        d_loss = (loss_discriminator_fake + loss_discriminator_real) / 2\n",
    "        avg_dloss = avg_dloss + (d_loss - avg_dloss) / (i + 1)\n",
    "        avg_loss = avg_loss + (losses_composite[0] - avg_loss) / (i + 1)\n",
    "        print('total loss:' + str(avg_loss) + ' d_loss:' + str(avg_dloss))\n",
    "\n",
    "        #Save progress images of the generator output\n",
    "        if i % 1 == 0:\n",
    "            firstImage = featureNP[0]\n",
    "            saveProgressImage(generator, firstImage, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819788a",
   "metadata": {},
   "source": [
    "Results of the above GAN seem promising, I will probably test using it with an improved data set generated in the next notebook.\n",
    "\n",
    "# Using Simpler Loss Functions\n",
    "\n",
    "[In this paper](https://arxiv.org/pdf/1810.08217.pdf), the authors used the same U-Net generator model as Eric does in his GAN. They used the U-Net model to simulate fluid/air simulations, but trained it with a simple $L_1$-Norm loss function rather than using a discriminator. To replicate this, we can re-use eric's Unet generator model, but use a simple loss function rather than training a dscrimator model along with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790617a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we define the structure of the generator model\n",
    "def getInspiredGeneratorModel(imageWidth, imageHeight):\n",
    "    \n",
    "    #Input layer\n",
    "    inputs = layers.Input((imageHeight, imageWidth, 3))\n",
    "    \n",
    "    #A few convolutional layers with maxpooling\n",
    "    #This is the encoder part of the model that interprets the input image\n",
    "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    #Their last convolution layer in their encoder seems to have skip layer with noise introduced to it\n",
    "    #Noise is often used to avoid the overfiting of a machine learning model\n",
    "    noise = layers.Input((K.int_shape(conv5)[1], K.int_shape(conv5)[2], K.int_shape(conv5)[3]))\n",
    "    conv5 = layers.Concatenate()([conv5, noise])\n",
    "    \n",
    "    #From my understanding, upsampling is used to increase the weight of data in the minority class\n",
    "    #Skip layer (connects conv4 layer directly to up6 layer), and more convolutional layers\n",
    "    up6 = layers.Conv2D(512, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv5))\n",
    "    merge6 = layers.Concatenate()([conv4, up6])\n",
    "    conv6 = layers.Conv2D(512, 3, activation='relu', padding='same')(merge6)\n",
    "    conv6 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
    "    \n",
    "    up7 = layers.Conv2D(256, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv6))\n",
    "    merge7 = layers.Concatenate()([conv3, up7])\n",
    "    conv7 = layers.Conv2D(256, 3, activation='relu', padding='same')(merge7)\n",
    "    conv7 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    up8 = layers.Conv2D(128, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv7))\n",
    "    merge8 = layers.Concatenate()([conv2, up8])\n",
    "    conv8 = layers.Conv2D(128, 3, activation='relu', padding='same')(merge8)\n",
    "    conv8 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
    "    \n",
    "    up9 = layers.Conv2D(64, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv8))\n",
    "    merge9 = layers.Concatenate()([conv1, up9])\n",
    "    conv9 = layers.Conv2D(64, 3, activation='relu', padding='same')(merge9)\n",
    "    conv9 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
    "    conv9 = layers.Conv2D(32, 3, activation='relu', padding='same')(conv9)\n",
    "    conv10 = layers.Conv2D(3, 1, activation='tanh')(conv9)\n",
    "    \n",
    "    #Create and return the final model\n",
    "    model = Model(inputs=[inputs, noise], outputs=conv10)\n",
    "    #model = Model(inputs=[inputs], outputs=conv10)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf7cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image parameters\n",
    "batch_size = 4\n",
    "imageWidth = 256\n",
    "imageHeight = 512\n",
    "\n",
    "#Choose image data directory depending whether we want to use all data, or a small subset for debugging.\n",
    "useAllData = False\n",
    "if useAllData:\n",
    "    featuresDir = './ImageTrainingData/Features'\n",
    "    targetsDir = './ImageTrainingData/Targets'\n",
    "else:\n",
    "    featuresDir = './SmallImageTrainingDataset/Features'\n",
    "    targetsDir = './SmallImageTrainingDataset/Targets'\n",
    "\n",
    "\n",
    "#Load data and extract a batch of each dataset\n",
    "featureData = getBatchDataset(featuresDir, imageHeight, imageWidth, batch_size)\n",
    "targetData = getBatchDataset(targetsDir, imageHeight, imageWidth, batch_size)\n",
    "featureNP = batchDatasetToNumpy(featureData)\n",
    "targetNP = batchDatasetToNumpy(targetData)\n",
    "\n",
    "\n",
    "#U-Net generator model\n",
    "generator = getInspiredGeneratorModel(imageWidth, imageHeight)\n",
    "mseLoss = tf.keras.losses.MeanSquaredError()\n",
    "generator.compile(optimizer='adam', loss=mseLoss, metrics=['mean_absolute_error'])\n",
    "\n",
    "#Save a generated image to track progress before training\n",
    "firstImage = featureNP[0]\n",
    "saveProgressImage(generator, firstImage, 0)\n",
    "\n",
    "#Main training loop\n",
    "for i in range(nSteps):\n",
    "    \n",
    "    #Chose a random selection of images for this batch, and generate required data for training\n",
    "    feats, realLabels, targs = generateRealSamples(featureNP, targetNP, batchSize, nXPatches, nYPatches)\n",
    "    #fakeOutput, fakeLabels = generateFakeSamples(generator, feats, nXPatches, nYPatches)\n",
    "    \n",
    "    w_noise = np.random.normal(0, 1, (batch_size, 32, 16, 1024))\n",
    "    losses = generator.train_on_batch([feats, w_noise], y=targs)\n",
    "    \n",
    "    print('Losses: {}'.format(losses))\n",
    "    \n",
    "    #Save progress images of the generator output\n",
    "    if i % 1 == 0:\n",
    "        firstImage = featureNP[0]\n",
    "        saveProgressImage(generator, firstImage, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9c0d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the test feature image, we save generator output images to view the progress of it's training\n",
    "def saveProgressImage(generator, testImage, iteration, dirName='./ProgressImages/Progress{}.png'):\n",
    "    w_noise = np.random.normal(0, 1, (1, 32, 16, 1024))\n",
    "    generatedImage = generator([firstImage[np.newaxis], w_noise], training=False)[0]\n",
    "    generatedImage = generatedImage.numpy()\n",
    "    generatedImage -= np.min(generatedImage)\n",
    "    generatedImage /= np.max(generatedImage)\n",
    "    generatedImage *= 256\n",
    "\n",
    "    img = Image.fromarray(generatedImage.astype(np.uint8))\n",
    "    img.save(dirName.format(iteration))\n",
    "\n",
    "firstImage = featureNP[0]\n",
    "saveProgressImage(generator, firstImage, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e2662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstImage = featureNP[0]\n",
    "\n",
    "w_noise = np.random.normal(0, 1, (1, 32, 16, 1024))\n",
    "generatedImage = generator([firstImage[np.newaxis], w_noise], training=False)[0]\n",
    "\n",
    "print(generatedImage.shape)\n",
    "print(np.max(generatedImage))\n",
    "print(np.min(generatedImage))\n",
    "print(np.max(firstImage))\n",
    "\n",
    "plt.imshow(generatedImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f36564c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bef5514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c12c82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69767edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the test feature image, we save generator output images to view the progress of it's training\n",
    "def saveProgressImage(generator, testImage, iteration, dirName='./ProgressImages/Progress{}.png'):\n",
    "    generatedImage = generator(firstImage[np.newaxis], training=False)[0]\n",
    "    generatedImage = generatedImage.numpy()\n",
    "    generatedImage -= np.min(generatedImage)\n",
    "    generatedImage /= np.max(generatedImage)\n",
    "    generatedImage *= 256\n",
    "\n",
    "    img = Image.fromarray(generatedImage.astype(np.uint8))\n",
    "    img.save(dirName.format(iteration))\n",
    "\n",
    "firstImage = featureNP[0]\n",
    "saveProgressImage(generator, firstImage, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40434313",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstImage = featureNP[0]\n",
    "generatedImage = generator(firstImage[np.newaxis], training=False)[0]\n",
    "generatedImage = generatedImage.numpy()\n",
    "print(np.min(generatedImage, axis=2).shape)\n",
    "\n",
    "generatedImage -= np.min(generatedImage)\n",
    "generatedImage /= np.max(generatedImage)\n",
    "generatedImage *= 256\n",
    "\n",
    "img = Image.fromarray(generatedImage.astype(np.uint8))\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f9fa8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d194d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3af94b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230ecd26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83905ffd",
   "metadata": {},
   "source": [
    "Loss functions for measuring how well our models are performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function for the discriminator model\n",
    "def inspiredDiscriminatorLoss(realOutput, fakeOutput):\n",
    "    crossEntropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    realLoss = crossEntropy(tf.ones_like(realOutput), realOutput)\n",
    "    fakeLoss = crossEntropy(tf.zeros_like(fakeOutput), fakeOutput)\n",
    "    totalLoss = realLoss + fakeLoss\n",
    "    return totalLoss\n",
    "\n",
    "#Loss function for the generator model\n",
    "def inspiredGeneratorLoss(fakeOutput):\n",
    "    crossEntropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    return crossEntropy(tf.ones_like(fakeOutput), fakeOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440d6087",
   "metadata": {},
   "source": [
    "Make sure that our loss functions are return results of a reasonable type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b57609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data and extract a batch\n",
    "featureData = getBatchDataset(featuresDir, imageHeight, imageWidth, batch_size)\n",
    "targetData = getBatchDataset(targetsDir, imageHeight, imageWidth, batch_size)\n",
    "featureBatch = next(iter(featureData))\n",
    "targetBatch = next(iter(targetData))\n",
    "\n",
    "#Generate next simulation iterator using our currently untrained generator model\n",
    "generator = getInspiredGeneratorModel(imageWidth, imageHeight)\n",
    "generatedImageBatch = generator(featureBatch, training=False)\n",
    "ones = tf.ones_like(generatedImageBatch)\n",
    "\n",
    "#Pass real and fake images to discriminator\n",
    "real_output = discriminator((featureBatch, targetBatch), training=False)\n",
    "fake_output = discriminator((featureBatch, generatedImageBatch), training=False)\n",
    "\n",
    "#Get losses\n",
    "discriminatorLoss = inspiredDiscriminatorLoss(real_output, fake_output)\n",
    "generatorLoss = inspiredGeneratorLoss(fake_output)\n",
    "\n",
    "\n",
    "\n",
    "print(generatorLoss.numpy())\n",
    "print(discriminatorLoss.numpy())\n",
    "\n",
    "image = generatedImageBatch[0].numpy()\n",
    "oneImage = ones[0].numpy()\n",
    "\n",
    "plt.imshow(oneImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f0fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the training loop for a single training epoch\n",
    "@tf.function\n",
    "def trainInspiredNetwork(inputBatch, outputBatch, generator, discriminator):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        \n",
    "        #Generate next simulation iterator using our generator model\n",
    "        generatedImages = generator(inputBatch, training=True)\n",
    "        \n",
    "        #Pass real and fake images to discriminator\n",
    "        real_output = discriminator((inputBatch, outputBatch), training=True)\n",
    "        fake_output = discriminator((inputBatch, generatedImages), training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "#Run the training algorithm for multiple epochs\n",
    "def train(featureData, targetData, epochs, generator, discriminator):\n",
    "    checkpoint = createTrainingCheckpoint()\n",
    "    testBatch = next(iter(featureData))\n",
    "    generate_and_save_images(generator, 0, testBatch)\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        for inputBatch, outputBatch in zip(featureData, targetData):\n",
    "            #print('Blah')\n",
    "            trainInspiredNetwork(inputBatch, outputBatch, generator, discriminator)\n",
    "\n",
    "        #Produce images to visualize the training progress\n",
    "        generate_and_save_images(generator, epoch + 1, testBatch)\n",
    "\n",
    "        #Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    #Generate after the final epoch\n",
    "    generate_and_save_images(generator, epochs, testBatch)\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    predictions = model(test_input, training=False)\n",
    "    image = predictions[0].numpy() * 255\n",
    "    plt.imshow(image.astype(np.uint8))\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()\n",
    "    \n",
    "def createTrainingCheckpoint(checkpoint_dir='./training_checkpoints'):\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    checkpoint = tf.train.Checkpoint(generator_optimizer=generatorOptimizer,\n",
    "                                     discriminator_optimizer=discriminatorOptimizer,\n",
    "                                     generator=generator,\n",
    "                                     discriminator=discriminator)\n",
    "    return checkpoint\n",
    "\n",
    "#Adam optimizers for tuning variables in our models during training \n",
    "generatorOptimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminatorOptimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee89a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "#Load data\n",
    "featureData = getBatchDataset(featuresDir, imageHeight, imageWidth, batch_size)\n",
    "targetData = getBatchDataset(targetsDir, imageHeight, imageWidth, batch_size)\n",
    "\n",
    "#Create models\n",
    "generator = getInspiredGeneratorModel(imageWidth, imageHeight)\n",
    "discriminator = getInspiredDiscriminatorModel(imageHeight, imageWidth)\n",
    "\n",
    "print(featureData)\n",
    "print(featureData.take(10))\n",
    "\n",
    "#inputBatch = next(iter(featureData))\n",
    "#generate_and_save_images(generator, 0 + 1, inputBatch)\n",
    "\n",
    "#Train our models\n",
    "#train(featureData, targetData, epochs, generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab8a0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
