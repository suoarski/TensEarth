{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7aed799",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In the previous notebook, we used perlin noise to randomly generate input data to run GOSPL simulations with. We then ran hundred/thousands of GOSPL trials and created a training data set that can be used to train various neural network (NN) models. In this notebook, we will train NN models to replicate GOSPL simulation.\n",
    "\n",
    "# Imports and Usefull Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d76a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from keras.initializers import RandomNormal\n",
    "\n",
    "# Load feature and target data\n",
    "def loadDataset(dataSetDir='./Data/TrainingDataSets/gosplData.npz'):\n",
    "    dataSet = np.load(dataSetDir)\n",
    "    features = dataSet['features']\n",
    "    targets = dataSet['targets']\n",
    "    return features, targets\n",
    "\n",
    "#Coordinate transformation from cartesian to polar\n",
    "def cartesianToPolarCoords(XYZ, useLonLat=True):\n",
    "    X, Y, Z = XYZ[:, 0], XYZ[:, 1], XYZ[:, 2]\n",
    "    R = (X**2 + Y**2 + Z**2)**0.5\n",
    "    theta = np.arctan2(Y, X)\n",
    "    phi = np.arccos(Z / R)\n",
    "\n",
    "    # Return results either in spherical polar or leave it in radians\n",
    "    if useLonLat == True:\n",
    "        theta, phi = np.degrees(theta), np.degrees(phi)\n",
    "        lon, lat = theta - 180, 90 - phi\n",
    "        lon[lon < -180] = lon[lon < -180] + 360\n",
    "        return R, lon, lat\n",
    "    else:\n",
    "        return R, theta, phi\n",
    "\n",
    "#Coordinate transformation from spherical polar to cartesian\n",
    "def polarToCartesian(radius, theta, phi, useLonLat=True):\n",
    "    if useLonLat == True:\n",
    "        theta, phi = np.radians(theta+180.), np.radians(90. - phi)\n",
    "    X = radius * np.cos(theta) * np.sin(phi)\n",
    "    Y = radius * np.sin(theta) * np.sin(phi)\n",
    "    Z = radius * np.cos(phi)\n",
    "\n",
    "    # Return data either as a list of XYZ coordinates or as a single XYZ coordinate\n",
    "    if (type(X) == np.ndarray):\n",
    "        return np.stack((X, Y, Z), axis=1)\n",
    "    else:\n",
    "        return np.array([X, Y, Z])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696283b5",
   "metadata": {},
   "source": [
    "# Visualizing the Training Data\n",
    "\n",
    "Before we continue, we visualize the data to make sure that they are in the correct format and haven't been corrupted along the way. These functions will also be usefull to visualize the output of the tensorflow models later on.\n",
    "\n",
    "In the code bellow, we provide a visualization of the data in the image format. This format is more representative of how tensorflow will actually see the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab052b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an image representation of a sample image\n",
    "def visualizeAsImages(featureImage, targetImage):\n",
    "    featureImage -= np.min(featureImage, axis=(0, 1))\n",
    "    featureImage /= np.max(featureImage, axis=(0, 1))\n",
    "    targetImage -= np.min(targetImage, axis=(0, 1))\n",
    "    targetImage /= np.max(targetImage, axis=(0, 1))\n",
    "    #targetImage[:, :, 2] **= 0.125\n",
    "\n",
    "    #Set up plotting figure\n",
    "    fig, axis = plt.subplots(1, 3)\n",
    "    fig.set_figheight(12)\n",
    "    fig.set_figwidth(18)\n",
    "    axis[0].imshow(featureImage[:, :, 0])\n",
    "    axis[1].imshow(featureImage[:, :, 1])\n",
    "    axis[2].imshow(targetImage)\n",
    "    axis[0].set_title('Initial Elevations')\n",
    "    axis[1].set_title('Tectonic Uplift')\n",
    "    axis[2].set_title('Combined Output Data')\n",
    "    axis[0].set_xticks([])\n",
    "    axis[0].set_yticks([])\n",
    "    axis[1].set_xticks([])\n",
    "    axis[1].set_yticks([])\n",
    "    axis[2].set_xticks([])\n",
    "    axis[2].set_yticks([])\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    return fig, axis\n",
    "\n",
    "# Load dataset and visualize a sample from it\n",
    "features, targets = loadDataset('./Data/TrainingDataSets/SmallExampleDataset.npz')\n",
    "fig, axis = visualizeAsImages(features[0], targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3226d69",
   "metadata": {},
   "source": [
    "Although the data used will be in an image format, it is meant to represent a spherical planet. The code bellow provides a spherical representation of what the data looks like, with an exagerated terrain based on the final elevations. This function will also be compatible with the outputs of the tensorflow models.\n",
    "\n",
    "In the top left corner of the ITK plotter window, there is a dropdown meny represented by 3 lines. Within this dropdown menu, we can chose which parameter to color code the mesh with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926cf068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pyvista mesh object with exagerated terrains and data attached to it\n",
    "def createExageratedMesh(featureSample, targetSample, amplificationFactor=90, earthRadius=6378137):\n",
    "    \n",
    "    # Reshape sample data and re-insert placeholder values (zeros) for north and south poles\n",
    "    imageDims = np.array(features.shape[1:3])\n",
    "    featureSample = featureSample.reshape(imageDims[0] * imageDims[1], 2)\n",
    "    featureSample = np.insert(featureSample, 0, 0, axis=0)\n",
    "    featureSample = np.insert(featureSample, 0, 0, axis=0)\n",
    "    targetSample = targetSample.reshape(imageDims[0] * imageDims[1], 3)\n",
    "    targetSample = np.insert(targetSample, 0, 0, axis=0)\n",
    "    targetSample = np.insert(targetSample, 0, 0, axis=0)\n",
    "\n",
    "    # Create a planet mesh with exagerated final elevations\n",
    "    uvSphere = pv.Sphere(theta_resolution=imageDims[0], phi_resolution=imageDims[1]+2)\n",
    "    r, lon, lat = cartesianToPolarCoords(uvSphere.points)\n",
    "    exegeratedRadius = earthRadius + amplificationFactor * targetSample[:, 0]\n",
    "    exageratedXYZ = polarToCartesian(exegeratedRadius, lon, lat)\n",
    "    exageratedMesh = pv.PolyData(exageratedXYZ.T, uvSphere.faces)\n",
    "\n",
    "    # Attach data to the exagerated mesh\n",
    "    exageratedMesh['Initial Elevations'] = featureSample[:, 0]\n",
    "    exageratedMesh['Tectonic Uplift'] = featureSample[:, 1]\n",
    "    exageratedMesh['Final Elevations'] = targetSample[:, 0]\n",
    "    exageratedMesh['Erosion Deposition'] = targetSample[:, 1]\n",
    "    exageratedMesh['Flow Accumilation'] = targetSample[:, 2]**0.125\n",
    "    return exageratedMesh\n",
    "\n",
    "# Create exagerated mesh\n",
    "sampleNumber = 10\n",
    "exageratedMesh = createExageratedMesh(features[sampleNumber], targets[sampleNumber])\n",
    "\n",
    "# Plot the results\n",
    "plotter = pv.PlotterITK()\n",
    "plotter.add_mesh(exageratedMesh, scalars='Final Elevations')\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ee5d56",
   "metadata": {},
   "source": [
    "# Sigmoids and Logit Functions\n",
    "\n",
    "Machine learning models generally don't perform well when the training data is within some random range, instead we want all data to be within a range of $[-1, 1]$, or sometimes within $[0, 1]$. We also don't want to use the standard normalization technique, because this would artificially introduce a lower and upper bound on our data. We can use the sigmoid function to bring the data into the range of $[0, 1]$, and a logit function to bring it back to the original data domain.\n",
    "\n",
    "The usual sigmoid function and logit is given bellow. Here $\\alpha$ is the gain, and corresponds to the steepness of the sigmoid function.\n",
    "\n",
    "$$\\text{sigmoid}(x, \\alpha, \\mu) = \\frac{1}{1 + e^{-\\alpha (x - \\mu)}}$$\n",
    "\n",
    "$$\\text{logit}(x, \\alpha, \\mu) = \\mu + \\log \\Big( {\\frac{x}{1 - x}} \\Big) \\Big/ \\alpha$$\n",
    "\n",
    "We want to chose a gain $\\alpha$ and centre $\\mu$ such that most of the variance in our data falls within the changing region of the sigmoid function. Therefore, we calculate a gain and centre based on a specified range in which the data varies by about 2 standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23aaa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to bring data to 0-1\n",
    "def sigmoid(x, minMax=np.array([-1, 1])):\n",
    "    centre = (minMax[0] + minMax[1]) / 2\n",
    "    gain = 8 / (minMax[1] - minMax[0])\n",
    "    return 1 / (1 + np.exp(- gain * (x - centre)))\n",
    "\n",
    "# Inverse function to bring data back to original domain\n",
    "def logit(x, minMax=np.array([-1, 1])):\n",
    "    centre = (minMax[0] + minMax[1]) / 2\n",
    "    gain = 8 / (minMax[1] - minMax[0])\n",
    "    return centre + np.log(x / (1 - x)) / gain\n",
    "\n",
    "# Create XY data to demonstrate functions with\n",
    "minMax = np.array([-4, 4])\n",
    "x = np.arange(-8, 8, 0.01)\n",
    "y = sigmoid(x, minMax=minMax)\n",
    "a = np.arange(0.01, 0.99, 0.01)\n",
    "b = logit(a, minMax=minMax)\n",
    "\n",
    "# Plot functions\n",
    "fig, axis = plt.subplots(1, 2)\n",
    "fig.set_figheight(4)\n",
    "fig.set_figwidth(16)\n",
    "axis[0].plot(x, y)\n",
    "axis[1].plot(a, b)\n",
    "axis[0].set_title('Sigmoid')\n",
    "axis[1].set_title('Logit')\n",
    "axis[0].grid()\n",
    "axis[1].grid()\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fdc121",
   "metadata": {},
   "source": [
    "When using the above functions, we want to choose an appropriate range for each variable to avoid all values being too close to 0 or 1, but rather something inbetween. Otherwise the neural network may not *see* the relevant details of our data set.\n",
    "\n",
    "We can use the figures bellow to figure out what range our data typically lies within, and decide what gain is suitable for each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940cbd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets = loadDataset(dataSetDir='./Data/TrainingDataSets/SmallerDataSet10Examples.npz')\n",
    "feats = features[0:5].reshape(5 * 512 * 256, 2)\n",
    "targs = targets[0:5].reshape(5 * 512 * 256, 3)\n",
    "\n",
    "fig, axis = plt.subplots(5, 1)\n",
    "fig.set_figheight(12)\n",
    "fig.set_figwidth(16)\n",
    "axis[0].plot(feats[:, 0], linewidth=0.05)\n",
    "axis[1].plot(feats[:, 1], linewidth=0.05)\n",
    "axis[2].plot(targs[:, 0], linewidth=0.05)\n",
    "axis[3].plot(targs[:, 1], linewidth=0.05)\n",
    "axis[4].plot(targs[:, 2]**0.25, linewidth=0.05)\n",
    "axis[0].set_title('Min: {}, Max: {}'.format(np.min(feats[:, 0]), np.max(feats[:, 0])))\n",
    "axis[1].set_title('Min: {}, Max: {}'.format(np.min(feats[:, 1]), np.max(feats[:, 1])))\n",
    "axis[2].set_title('Min: {}, Max: {}'.format(np.min(targs[:, 0]), np.max(targs[:, 0])))\n",
    "axis[3].set_title('Min: {}, Max: {}'.format(np.min(targs[:, 1]), np.max(targs[:, 1])))\n",
    "axis[4].set_title('Min: {}, Max: {}'.format(np.min(targs[:, 2]**0.125), np.max(targs[:, 2]**0.125)))\n",
    "for i in range(5):\n",
    "    axis[i].set_xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brings data to range of 0-1 without imposing a strict upper bounds\n",
    "def passDataThroughSigmoid(features, targets, \n",
    "                           minMaxes = np.array([[-4000, 1000],\n",
    "                                                [0, 0.0002],\n",
    "                                                [0, 12000],\n",
    "                                                [-1000, 3000],\n",
    "                                                [0, 1000]])):\n",
    "    newFeatures, newTargets = [], []\n",
    "    newFeatures.append(sigmoid(features.T[0], minMax=minMaxes[0]))\n",
    "    newFeatures.append(sigmoid(features.T[1], minMax=minMaxes[1]))\n",
    "    newTargets.append(sigmoid(targets.T[0], minMax=minMaxes[2]))\n",
    "    newTargets.append(sigmoid(targets.T[1], minMax=minMaxes[3]))\n",
    "    newTargets.append(sigmoid(targets.T[2]**0.25, minMax=minMaxes[4]))\n",
    "    return np.array(newFeatures).T, np.array(newTargets).T\n",
    "\n",
    "# Brings the output of the model back into the original domain\n",
    "def passDataThroughLogit(features, targets, \n",
    "                           minMaxes = np.array([[-4000, 1000],\n",
    "                                                [0, 0.0002],\n",
    "                                                [0, 12000],\n",
    "                                                [-1000, 3000],\n",
    "                                                [0, 1000]])):\n",
    "    newFeatures, newTargets = [], []\n",
    "    newFeatures.append(logit(features.T[0], minMax=minMaxes[0]))\n",
    "    newFeatures.append(logit(features.T[1], minMax=minMaxes[1]))\n",
    "    newTargets.append(logit(targets.T[0], minMax=minMaxes[2]))\n",
    "    newTargets.append(logit(targets.T[1], minMax=minMaxes[3]))\n",
    "    newTargets.append(logit(targets.T[2], minMax=minMaxes[4])**4)\n",
    "    return np.array(newFeatures).T, np.array(newTargets).T\n",
    "\n",
    "# Specify min max ranges for sigmoid and logit functions for all parameters\n",
    "minMaxes = np.array([\n",
    "    [-4000, 1000],\n",
    "    [0, 0.0002],\n",
    "    [0, 12000],\n",
    "    [-1000, 3000],\n",
    "    [0, 1000]])\n",
    "\n",
    "#features, targets = loadDataset()\n",
    "newFeatures, newTargets = passDataThroughSigmoid(features, targets, minMaxes)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(newTargets[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-school",
   "metadata": {},
   "source": [
    "In the future, it would be desirable to have this sigmoid and logit transformation take place within the tensorflow model itself such that the gain and centre are trainable parameters.\n",
    "\n",
    "# The Generator - Unet Model\n",
    "\n",
    "In the context of a GAN, the generator will attempt will take initial topography and tectonic uplift as inputs, and attempt to generate a *fake* output that fools the discriminateor into thinking it came from the *real* data set, which in our case has been generated by GOSPL. In previous notebooks, we found the Unet model seems to be a promising neural network architecture for our generator. \n",
    "\n",
    "The Unet model is a NN model in which almost all layers are convolutional. The image bellow ([taken from here](https://upload.wikimedia.org/wikipedia/commons/2/2b/Example_architecture_of_U-Net_for_producing_k_256-by-256_image_masks_for_a_256-by-256_RGB_image.png)) is an example diagram of how a Unet model can be structured. It is composed of a series of downsampling (aka encoding) layers, in which convolution and pooling layers reduce the data in it's image dimension, but the number of convolutional filters increase in each layer. Noise is then introduced in the botom most layer, as it prevents overfitting of the data (Why noise helps is still an open research question).\n",
    "\n",
    "A series of upsampling convolutional layers are used to bring the data back into it's desired image dimensions. The output of each upsampling layer is merged with the corresponding downsampling layer of the same depth, which allows data to bypass the deeper encoding/decoding layers.\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"../NotebookImages/ExampleUnet.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "The code bellow is taken and modified from [*nanoxas/sketch-to-terrain* Github](https://github.com/nanoxas/sketch-to-terrain/blob/master/model.py), where [Eric Guerin et. al.](https://hal.archives-ouvertes.fr/hal-01583706/file/tog.pdf) had a similar goal in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we define the structure of the generator Unet model\n",
    "def getUnet(imageDims):\n",
    "    imageWidth, imageHeight = imageDims[1], imageDims[0]\n",
    "    \n",
    "    #Input layer\n",
    "    inputs = layers.Input((imageHeight, imageWidth, 2))\n",
    "    \n",
    "    #A few convolutional layers with maxpooling\n",
    "    #This is the encoder part of the model that interprets the input image\n",
    "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    #Their last convolution layer in their encoder seems to have skip layer with noise introduced to it\n",
    "    #Noise is often used to avoid the overfiting of a machine learning model\n",
    "    noise = layers.Input((K.int_shape(conv5)[1], K.int_shape(conv5)[2], K.int_shape(conv5)[3]))\n",
    "    conv5 = layers.Concatenate()([conv5, noise])\n",
    "    \n",
    "    #From my understanding, upsampling is used to increase the weight of data in the minority class\n",
    "    #Skip layer (connects conv4 layer directly to up6 layer), and more convolutional layers\n",
    "    up6 = layers.Conv2D(512, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv5))\n",
    "    merge6 = layers.Concatenate()([conv4, up6])\n",
    "    conv6 = layers.Conv2D(512, 3, activation='relu', padding='same')(merge6)\n",
    "    conv6 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
    "    \n",
    "    up7 = layers.Conv2D(256, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv6))\n",
    "    merge7 = layers.Concatenate()([conv3, up7])\n",
    "    conv7 = layers.Conv2D(256, 3, activation='relu', padding='same')(merge7)\n",
    "    conv7 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    up8 = layers.Conv2D(128, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv7))\n",
    "    merge8 = layers.Concatenate()([conv2, up8])\n",
    "    conv8 = layers.Conv2D(128, 3, activation='relu', padding='same')(merge8)\n",
    "    conv8 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
    "    \n",
    "    up9 = layers.Conv2D(64, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv8))\n",
    "    merge9 = layers.Concatenate()([conv1, up9])\n",
    "    conv9 = layers.Conv2D(64, 3, activation='relu', padding='same')(merge9)\n",
    "    conv9 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
    "    conv9 = layers.Conv2D(32, 3, activation='relu', padding='same')(conv9)\n",
    "    conv10 = layers.Conv2D(3, 1, activation='tanh')(conv9)\n",
    "    \n",
    "    #Create and return the final model\n",
    "    model = Model(inputs=[inputs, noise], outputs=conv10)\n",
    "    return model\n",
    "\n",
    "# Create a generator model and print a summary of it's layers\n",
    "#features, targets = loadDataset()\n",
    "imageDims = features.shape[1:3]\n",
    "generator = getUnet(imageDims)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-portrait",
   "metadata": {},
   "source": [
    "To check that the generator model is working, we initialise an untrained generator and pass through some example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and normalize\n",
    "newFeatures, newTargets = passDataThroughSigmoid(features, targets, minMaxes)\n",
    "newFeatures, newTargets = features*2 - 1, targets*2 - 1\n",
    "\n",
    "# Initialise a generator model\n",
    "imageDims = newFeatures.shape[1:3]\n",
    "generator = getUnet(imageDims)\n",
    "\n",
    "# Pass through some example data and viusualize the untrained model\n",
    "batchSize = 2\n",
    "featuresBatch = newFeatures[0:batchSize]\n",
    "noise = np.random.normal(0, 1, (batchSize, 32, 16, 1024))\n",
    "generatedImageBatch = generator([featuresBatch, noise], training=False).numpy()\n",
    "print('Min: {}, Max: {}'.format(np.min(generatedImageBatch), np.max(generatedImageBatch)))\n",
    "fig, axis = visualizeAsImages(featuresBatch[0], generatedImageBatch[0])\n",
    "axis[2].set_title('Generated Output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-signature",
   "metadata": {},
   "source": [
    "# Discriminator Model\n",
    "\n",
    "The discriminator model attempts to distinguish *real* input produced by GOSPL from *fake* inputs produced by the generator model discussed above. It will take two images as inputs, one which is the same input given to the generator model, and one that is either generated by GOSPL (real) or by our generator model (fake). It's output will be a 2D array of predictions ranging from 0 to 1, based on how confident it is that each patch of the image is real or fake. Again, the code bellow has been taken and modified from *Eric Guerin et. al.*'s implementation. The structure of the discriminator is a series of downsampling convolutional layers.\n",
    "\n",
    "In the combined model, the performance of the discriminator will be measured by a loss function, which uses binary cross entropy to measure how similar the discriminator output is to the ground truth. During training, the weights of the discriminator model will be adjusted to minimize this loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-flavor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getDiscriminatorModel(imageDims):\n",
    "    imageWidth, imageHeight = imageDims[1], imageDims[0]\n",
    "    \n",
    "    #Create inputs of initial and generated image to discrimate, and combine them\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    initialImage = layers.Input(shape=(imageHeight, imageWidth, 2))\n",
    "    generatedImage = layers.Input((imageHeight, imageWidth, 3))\n",
    "    combinedImages = layers.Concatenate()([initialImage, generatedImage])\n",
    "    \n",
    "    #Main structure of the discriminator neural network model\n",
    "    d = layers.Conv2D(64/4, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(combinedImages)\n",
    "    d = layers.LeakyReLU(alpha=0.2)(d)\n",
    "    d = layers.Conv2D(128/4, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(d)\n",
    "    d = layers.LeakyReLU(alpha=0.2)(d)\n",
    "    d = layers.Conv2D(256/4, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(d)\n",
    "    d = layers.LeakyReLU(alpha=0.2)(d)\n",
    "    d = layers.Conv2D(512/2, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(d)\n",
    "    d = layers.LeakyReLU(alpha=0.2)(d)\n",
    "    d = layers.Conv2D(512/2, (4, 4), padding='same', kernel_initializer=init)(d)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(d)\n",
    "    output = layers.Conv2D(1, (4, 4), padding='same', activation='sigmoid', kernel_initializer=init)(d)\n",
    "    \n",
    "    #Create the model and return it\n",
    "    model = Model([initialImage, generatedImage], output)\n",
    "    return model\n",
    "\n",
    "# Create a discriminator model and print a summary of it's layers\n",
    "discriminator = getDiscriminatorModel(imageDims)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-barrier",
   "metadata": {},
   "source": [
    "Pass data through to discriminator to confirm it is indeed working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = discriminator((featuresBatch, generatedImageBatch))\n",
    "print(decision.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-wrong",
   "metadata": {},
   "source": [
    "# The Combined Model\n",
    "\n",
    "Using the generator and discriminator models above, we can combine the two into a single tensorflow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinedModel(generator, discriminator, imageDims):\n",
    "    discriminator.trainable = False\n",
    "    generatorInput = layers.Input(shape=(imageDims[0], imageDims[1], 2))\n",
    "    inputNoise = layers.Input(shape=(32, 16, 1024))\n",
    "    generatorOutput = generator([generatorInput, inputNoise])\n",
    "    discriminatorOutput = discriminator([generatorInput, generatorOutput])\n",
    "    model = Model(inputs=[generatorInput, inputNoise], outputs=[discriminatorOutput, generatorOutput])\n",
    "    return model\n",
    "\n",
    "#Create a mountDiscriminator and print summary of model\n",
    "mountDiscriminator = combinedModel(generator, discriminator, imageDims)\n",
    "mountDiscriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-reminder",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "Bellow are a few functions that will help with training our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and normalize it using sigmoids\n",
    "def loadAndPrepareData(dataSetDir='./Data/TrainingDataSets/Combined.npz'):\n",
    "    featuresOriginal, targetsOriginal = loadDataset(dataSetDir=dataSetDir)\n",
    "    features, targets = passDataThroughSigmoid(featuresOriginal, targetsOriginal)\n",
    "    return features*2 - 1, targets*2 - 1\n",
    "\n",
    "#Get number of patches in X and Y directions of the discriminator output\n",
    "def getNXYpatches(features, batchSize, generator, discriminator):\n",
    "    featuresBatch = features[0:batchSize]\n",
    "    w_noise = np.random.normal(0, 1, (batchSize, 32, 16, 1024))\n",
    "    generatedImageBatch = generator([featuresBatch, w_noise], training=False)\n",
    "    decision = discriminator((featuresBatch, generatedImageBatch))\n",
    "    nXPatches, nYPatches = decision.shape[1], decision.shape[2]\n",
    "    return nXPatches, nYPatches\n",
    "\n",
    "#Using the test feature image, we save generator output images to view the progress of it's training\n",
    "def saveProgressImage(generator, testImage, iteration, dirName='./ProgressImages/Progress{}.png'):\n",
    "    w_noise = np.random.normal(0, 1, (1, 32, 16, 1024))\n",
    "    generatedImage = generator([testImage[np.newaxis], w_noise], training=False)[0]\n",
    "    generatedImage = generatedImage.numpy()\n",
    "    generatedImage = ((generatedImage + 1) / 2) * 255\n",
    "    img = Image.fromarray(generatedImage.astype(np.uint8))\n",
    "    img.save(dirName.format(iteration))\n",
    "\n",
    "'''\n",
    "#Using the test feature image, we save generator output images to view the progress of it's training\n",
    "def saveProgressImage(generator, testImage, iteration, dirName='./ProgressImages/Progress{}.png'):\n",
    "    w_noise = np.random.normal(0, 1, (1, 32, 16, 1024))\n",
    "    generatedImage = generator([testImage[np.newaxis], w_noise], training=False)[0]\n",
    "    generatedImage = generatedImage.numpy()\n",
    "    generatedImage -= np.min(generatedImage)\n",
    "    generatedImage /= np.max(generatedImage)\n",
    "    generatedImage *= 256\n",
    "    img = Image.fromarray(generatedImage.astype(np.uint8))\n",
    "    img.save(dirName.format(iteration))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-scholarship",
   "metadata": {},
   "source": [
    "We create a class named GAN. It will load and prepare our data, set up our models and write relevant outputs to file in the appropriate output directories.\n",
    "\n",
    "It will initialise untrained generator and discriminator model, and compile them by specifying appropriate loss functions and optimization methods. A function is defined for creating real examples from the GOSPL training data, and another function is used to generate fake examples using the generator model. The discriminator will attempt to distinguish the two examples, and it's weights will be adjusted to improve at this task. The weights of the generator model will then be adjusted to improve it's ability to fool the discriminator.\n",
    "\n",
    "The function *saveModels* will write the weights of the model to file, which can be used to load the model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    \n",
    "    # Initialise GAN with various optional parameters\n",
    "    def __init__(self,\n",
    "                 batchSize = 4,\n",
    "                 avg_loss = 0,\n",
    "                 avg_dloss = 0,\n",
    "                 \n",
    "                 dataSetDir = './Data/TrainingDataSets/Combined.npz',\n",
    "                 modelsMainDir = './SavedTensorflowModels', \n",
    "                 trialDirFormat = '{}/Model{}'\n",
    "                ):\n",
    "        \n",
    "        # Store parameters as class properties\n",
    "        self.batchSize = batchSize\n",
    "        self.avg_loss = avg_loss\n",
    "        self.avg_dloss = avg_dloss\n",
    "        self.dataSetDir = dataSetDir\n",
    "        self.modelsMainDir = modelsMainDir\n",
    "        self.trialDirFormat = trialDirFormat\n",
    "        \n",
    "        \n",
    "        self.features, self.targets = loadAndPrepareData(dataSetDir=dataSetDir)\n",
    "        self.setUpModels(self.features.shape[1:3])\n",
    "        self.nXPatches, self.nYPatches = getNXYpatches(self.features, self.batchSize, self.generator, self.discriminator)\n",
    "        self.createModelsDir()\n",
    "    \n",
    "    # Do a single training step on all models\n",
    "    def doTrainingStep(self, iteration):\n",
    "    \n",
    "        # Create real examples from data set and fake examples from generator model\n",
    "        feats, realLabels, targs = self.generateRealSamples()\n",
    "        fakeOutput, fakeLabels = self.generateFakeSamples(feats)\n",
    "\n",
    "        # Training steps for composite and discriminator models\n",
    "        w_noise = np.random.normal(0, 1, (self.batchSize, 32, 16, 1024))\n",
    "        losses_composite = self.compositeModel.train_on_batch([feats, w_noise], [realLabels, targs])\n",
    "        loss_discriminator_fake = self.discriminator.train_on_batch([feats, fakeOutput], fakeLabels)\n",
    "        loss_discriminator_real = self.discriminator.train_on_batch([feats, targs], realLabels)\n",
    "        saveProgressImage(self.generator, self.features[0], iteration, dirName=self.progressImageDir+'/Progress{}.png')\n",
    "\n",
    "        # Print average loss so far\n",
    "        d_loss = (loss_discriminator_fake + loss_discriminator_real) / 2\n",
    "        self.avg_dloss += (d_loss - self.avg_dloss) / (i + 1)\n",
    "        self.avg_loss += (losses_composite[0] - self.avg_loss) / (i + 1)\n",
    "        print('total loss:' + str(self.avg_loss) + ' d_loss:' + str(self.avg_dloss))\n",
    "        print('Dloss: {}, GenLoss: {}'.format(d_loss, losses_composite[0]))\n",
    "    \n",
    "    # Set up models with loss functions and optimizers\n",
    "    def setUpModels(self, imageDims):\n",
    "        self.generator = getUnet(imageDims)\n",
    "        self.discriminator = getDiscriminatorModel(imageDims)\n",
    "        adamOptimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=adamOptimizer)\n",
    "        self.compositeModel = combinedModel(self.generator, self.discriminator, imageDims)\n",
    "        self.compositeModel.compile(loss=['binary_crossentropy', 'mae'], loss_weights=[1, 3], optimizer=adamOptimizer)\n",
    "    \n",
    "    # Randomly select a portion of feature data along with their corresponding target data\n",
    "    def generateRealSamples(self):\n",
    "        idx = np.random.randint(0, self.features.shape[0], self.batchSize)\n",
    "        feats = self.features[idx]\n",
    "        targs = self.targets[idx]\n",
    "        desiredDiscriminatorProbs = np.ones((self.batchSize, self.nXPatches, self.nYPatches, 1))\n",
    "        return feats, desiredDiscriminatorProbs, targs\n",
    "\n",
    "    # Generate fake images corresponding to the real targets\n",
    "    def generateFakeSamples(self, featureBatch):\n",
    "        w_noise = np.random.normal(0, 1, (featureBatch.shape[0], 32, 16, 1024))\n",
    "        fakeOutput = self.generator.predict([featureBatch, w_noise])\n",
    "        desiredDescriminatorProbs = np.zeros((len(fakeOutput), self.nXPatches, self.nYPatches, 1))\n",
    "        return fakeOutput, desiredDescriminatorProbs\n",
    "    \n",
    "    # Within the main tensorflow models directory, create and return a subdirectory for this particular model\n",
    "    def createModelsDir(self):\n",
    "        self.trialNumber = 0\n",
    "        self.trialDir = self.trialDirFormat.format(self.modelsMainDir, self.trialNumber)\n",
    "        while os.path.isdir(self.trialDir):\n",
    "            self.trialNumber += 1\n",
    "            self.trialDir = self.trialDirFormat.format(self.modelsMainDir, self.trialNumber)\n",
    "        self.progressImageDir = self.trialDir + '/ProgressImages'\n",
    "        os.mkdir(self.trialDir)\n",
    "        os.mkdir(self.progressImageDir)\n",
    "    \n",
    "    # Save the tensorflow model to file\n",
    "    def saveModels(self, iteration):\n",
    "        modelsSaveDir = self.trialDir + '/ModelsAtIteration{}'.format(iteration)\n",
    "        os.mkdir(modelsSaveDir)\n",
    "        self.generator.save(modelsSaveDir + '/generator.h5')\n",
    "        self.discriminator.save(modelsSaveDir + '/discriminator.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-activation",
   "metadata": {},
   "source": [
    "The code bellow will run the training of the GAN. It will create a new directory to save files to, and generate a progress image at each training iteration. Every N steps, it will also save the models weights to file.\n",
    "\n",
    "Note that as of now, the resulting model varies significantly in quality. This is due to local minima in the loss functions that the weights get stuck in, which prevents us from finding the global minium. During training, make sure to monitor the output progress images saved to file, to ensure that the model is indeed improving in quality.\n",
    "\n",
    "To stop the model from training, simply interupt the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "trainThisGan = False\n",
    "saveModelEveryNsteps = 200\n",
    "\n",
    "if trainThisGan:\n",
    "    #gan = GAN(dataSetDir='./Data/TrainingDataSets/Combined2.npz')\n",
    "    gan = GAN(dataSetDir='./Data/TrainingDataSets/gosplData.npz')\n",
    "    while True:\n",
    "        i += 1\n",
    "        gan.doTrainingStep(i)\n",
    "        if i % saveModelEveryNsteps == 0:\n",
    "            gan.saveModels(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-appendix",
   "metadata": {},
   "source": [
    "# Load and Run a Saved Tensorflow Model\n",
    "\n",
    "After training and saving various tensorflow models, we can load them using the code bellow. We are only saving the generator and discriminator components of the model, and not the overall combined model. These discarded components include loss functions, and optimization methods, which are only really required for training. If we would like to continue training a saved model, we will need to recompile them with the appropriate components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new GAN instance just like usual\n",
    "gan = GAN(dataSetDir='./Data/TrainingDataSets/gosplData.npz')\n",
    "\n",
    "# Load the weights to the generator and discriminator models\n",
    "generatorModelDir = './SavedTensorflowModels/Model0/ModelsAtIteration13200/generator.h5'\n",
    "discriminatorModelDir = './SavedTensorflowModels/Model0/ModelsAtIteration13200/discriminator.h5'\n",
    "gan.generator.load_weights(generatorModelDir)\n",
    "gan.discriminator.load_weights(discriminatorModelDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-experiment",
   "metadata": {},
   "source": [
    "After loading the above model, take a random sample from the training data set and it's input, and use it to generate an output that we can visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 2\n",
    "rand = np.random.randint(0, gan.features.shape[0] - batchSize)\n",
    "featuresBatch = gan.features[rand:rand+batchSize]\n",
    "noise = np.random.normal(0, 1, (batchSize, 32, 16, 1024))\n",
    "generatedImageBatch = gan.generator([featuresBatch, noise], training=False).numpy()\n",
    "fig, axis = visualizeAsImages(featuresBatch[0], generatedImageBatch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-cartridge",
   "metadata": {},
   "source": [
    "We would now like to see how the results look like on a spherical mesh, the format that we actually care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass a random input through the tensorflow model\n",
    "batchSize = 2\n",
    "rand = np.random.randint(0, gan.features.shape[0] - batchSize)\n",
    "featuresBatch = gan.features[rand:rand+batchSize]\n",
    "noise = np.random.normal(0, 1, (batchSize, 32, 16, 1024))\n",
    "generatedImageBatch = gan.generator([featuresBatch, noise], training=False).numpy()\n",
    "\n",
    "# Bring data from [-1, 1] to [0, 1]\n",
    "featureSample = (featuresBatch[0]+1)/2\n",
    "generatedImage = (generatedImageBatch[0]+1)/2\n",
    "\n",
    "# Add some padding to the data range [0, 1] (to avoid infinities)\n",
    "offset = 1e-7\n",
    "featuresOffseted = (featureSample + offset) / (1+2*offset)\n",
    "targetOffseted = (generatedImage + offset) / (1+2*offset)\n",
    "resultFeatures, resultsTarget = passDataThroughLogit(featuresOffseted, targetOffseted)\n",
    "\n",
    "# Visualise results as a mesh\n",
    "plotter = pv.PlotterITK()\n",
    "resultsMesh = createExageratedMesh(resultFeatures, resultsTarget, amplificationFactor=120, earthRadius=6378137)\n",
    "plotter.add_mesh(resultsMesh, scalars=\"Final Elevations\")\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-handbook",
   "metadata": {},
   "source": [
    "From the above results, we can see that our model is capable of producing an output that somewhat resembles GOSPL simulation, but some clear defects are visible. The river networks are not that visible, and instead we have a weirdly rough topography.\n",
    "\n",
    "**How to Improve:** A potential major source of error is the fact that our discriminator is working with data within the sigmoid transformed space used in preprocessing, and not within the original data domain. This means our generator model is learning how to generate fake examples within the sigmoid domain. To fix this, the discriminator needs to compare sample within the original data domain, and not the transformed space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-piece",
   "metadata": {},
   "source": [
    "# Testing Model on Unseen Input Data\n",
    "\n",
    "To trully test how well the model perform, we need to pass it some example input data that was not included in the training data. This is to make sure that the model generalizes well to unseen data, and avoid overfitting. We begin by redefining the noise functions used to generate the original input data. After randomly generating new input data, we pass it through the generator model, and visualise it as a spherical mesh.\n",
    "\n",
    "Our generator model seems to perform just as well on unseed inputs as it does with inputs included in the trianing stage, which means we do not have a problem of overfitting. To further test the capabilities of our model, we can change the noise parameters used to generate the initial topography and uplit. Doing this will test the model on noise frequencies that were not at all included in the training data.\n",
    "\n",
    "The model works surprising well on uplift generated by low frequency noise (change *octaveStepSize* to 0.8 within uplift noise parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample noise with multiple noise frequencies\n",
    "def sampleNoise(XYZ, noiseParameters):\n",
    "    noiseSum = np.zeros(XYZ.shape[0])\n",
    "    for i in range(noiseParameters[\"octaves\"]):\n",
    "        frequency =  noiseParameters[\"initialFrequency\"] * noiseParameters[\"octaveStepSize\"] ** i\n",
    "        noiseSum += samplePerlinNoise(XYZ, frequency=frequency) / (noiseParameters[\"amplitudeStepSize\"]*(i+1))\n",
    "\n",
    "    # Normalise noise and bring to desired noise range\n",
    "    minMax = noiseParameters[\"noiseMinMax\"]\n",
    "    noiseSum -= np.min(noiseSum)\n",
    "    noiseSum = noiseSum * (minMax[1] - minMax[0]) / np.max(noiseSum) + minMax[0]\n",
    "    return noiseSum\n",
    "\n",
    "# Given a list of XYZ coordinates, we sample a noise value at each point\n",
    "def samplePerlinNoise(XYZ, amplitude=1, frequency=4, offset=None):\n",
    "    if offset == None: #Random offset if none other is specified\n",
    "        offset = np.random.rand(3) * 100000\n",
    "    freq = (frequency, frequency, frequency)\n",
    "    noise = pv.perlin_noise(amplitude, freq, offset)\n",
    "    return np.array([noise.EvaluateFunction(xyz) for xyz in XYZ])\n",
    "\n",
    "\n",
    "# Parameters used to generate original input data\n",
    "earthRadius = 6378137\n",
    "sphereResolution = [512, 258]\n",
    "topographyNoiseParameters = {\n",
    "    \"octaves\" : 8,\n",
    "    \"octaveStepSize\" : 1.4,\n",
    "    \"amplitudeStepSize\" : 2.0,\n",
    "    \"initialFrequency\" : 0.000001,\n",
    "    \"noiseMinMax\" : np.array([-4000, 1000])}\n",
    "upliftNoiseParameters = {\n",
    "    \"octaves\" : 6,\n",
    "    \"octaveStepSize\" : 1.4, # Change this to significantly change noise frequencies of uplift\n",
    "    \"amplitudeStepSize\" : 2.4,\n",
    "    \"initialFrequency\" : 0.0000002,\n",
    "    \"noiseMinMax\" : np.array([0, 2000])}\n",
    "\n",
    "# Create a sphere and generate noise based inputs\n",
    "start = time.time()\n",
    "sphere = pv.Sphere(theta_resolution=sphereResolution[0], phi_resolution=sphereResolution[1], radius=earthRadius)\n",
    "topography = sampleNoise(sphere.points, topographyNoiseParameters)\n",
    "uplift = sampleNoise(sphere.points, upliftNoiseParameters)\n",
    "print('Took {} seconds to generate input data'.format(time.time() - start))\n",
    "start = time.time()\n",
    "\n",
    "# Preprocess initial topography input\n",
    "topoNormalized = topography.T[2:]\n",
    "topoNormalized = topoNormalized.reshape(512, 256)\n",
    "topoNormalized = sigmoid(topoNormalized, minMax=[-4000, 1000])\n",
    "topoNormalized = 2 * topoNormalized - 1\n",
    "\n",
    "# Preprocess tectonic uplift input\n",
    "upliftNormalized = uplift.T[2:] / 10000000\n",
    "upliftNormalized = upliftNormalized.reshape(512, 256)\n",
    "upliftNormalized = sigmoid(upliftNormalized, minMax=[0, 0.0002])\n",
    "upliftNormalized = 2 * upliftNormalized - 1\n",
    "\n",
    "# Pass randomly generated input data through the tensorflow model\n",
    "noise = np.random.normal(0, 1, (1, 32, 16, 1024))\n",
    "inputBatch = np.array([np.stack((topoNormalized, upliftNormalized), axis=-1)])\n",
    "print('Took {} seconds to preprocess data'.format(time.time() - start))\n",
    "start = time.time()\n",
    "\n",
    "generatedImageBatch = gan.generator([inputBatch, noise], training=False).numpy()\n",
    "print('Took {} seconds pass data through sigmoid'.format(time.time() - start))\n",
    "start = time.time()\n",
    "\n",
    "# Bring output data back to original data domain\n",
    "offset = 1e-2\n",
    "featureSample = (inputBatch[0]+1)/2\n",
    "generatedImage = (generatedImageBatch[0]+1)/2\n",
    "featuresOffseted = (featureSample + offset) / (1+2*offset)\n",
    "targetOffseted = (generatedImage + offset) / (1+2*offset)\n",
    "resultFeatures, resultsTarget = passDataThroughLogit(featuresOffseted, targetOffseted)\n",
    "print('Took {} seconds to postprocess data'.format(time.time() - start))\n",
    "\n",
    "# Plot the results\n",
    "plotter = pv.PlotterITK()\n",
    "resultsMesh = createExageratedMesh(resultFeatures, resultsTarget, amplificationFactor=120, earthRadius=6378137)\n",
    "plotter.add_mesh(resultsMesh, scalars=\"Final Elevations\")\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-observation",
   "metadata": {},
   "source": [
    "We can compare the above output with a real example generated by GOSPL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17299a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create exagerated mesh\n",
    "sampleNumber = 10\n",
    "features, targets = loadDataset('./Data/TrainingDataSets/SmallExampleDataset.npz')\n",
    "exageratedMesh = createExageratedMesh(features[sampleNumber], targets[sampleNumber])\n",
    "\n",
    "# Plot the results\n",
    "plotter = pv.PlotterITK()\n",
    "plotter.add_mesh(exageratedMesh, scalars='Final Elevations')\n",
    "plotter.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
